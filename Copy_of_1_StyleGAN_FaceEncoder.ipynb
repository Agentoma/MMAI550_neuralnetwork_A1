{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Agentoma/MMAI550_neuralnetwork_A1/blob/main/Copy_of_1_StyleGAN_FaceEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1oARVh4UVXz"
      },
      "source": [
        "# Finding a latent StyleGAN encoding of a face\n",
        "\n",
        "Modified from the Colab notebook assoicated with the video [Editing Faces using Artificial Intelligence](https://www.youtube.com/watch?v=dCKbRCUyop8) and the GitHub repository [StyleGAN Encoder - Pytorch Implementation](https://github.com/jacobhallberg/pytorch_stylegan_encoder).\n",
        "\n",
        "By Hjalmar K Turesson\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOUqT7K5f70t"
      },
      "source": [
        "# Part I: Encoding images into StyleGAN's latent space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ON6OfMkVTVl"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/1280/0*eeFaGLx96mlbQcrK.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKQomSbfVBct"
      },
      "source": [
        "## Before you move on, make sure you have GPU acceleration enabled:\n",
        "> ### Click 'Runtime' in the menu tab at the top\n",
        "> ### Click 'Change runtime type'\n",
        "> ### Make sure the hardware accelerator is set to 'GPU'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVGS1TnvVd35"
      },
      "source": [
        "## GitHub repositories\n",
        "\n",
        "We will use three GitHub repos to find the latent encoding of faces.\n",
        " * [StyleGAN Encoder - Pytorch Implementation](https://github.com/jacobhallberg/pytorch_stylegan_encoder.git): PyTorch version\n",
        " * [InterFace GAN](https://github.com/genforce/interfacegan.git)\n",
        " * [StyleGAN encoder](https://github.com/Puzer/stylegan-encoder): Older Tensorflow version from where we are going to use `align_images.py`\n",
        "\n",
        " ### Get StyleGAN Encoder - Pytorch Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkycNxEeUWBA",
        "outputId": "6aeb7f36-f029-4a60-c24b-6cb37ccf3b2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch_stylegan_encoder'...\n",
            "remote: Enumerating objects: 361, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 361 (delta 0), reused 0 (delta 0), pack-reused 358 (from 1)\u001b[K\n",
            "Receiving objects: 100% (361/361), 56.89 MiB | 19.22 MiB/s, done.\n",
            "Resolving deltas: 100% (185/185), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jacobhallberg/pytorch_stylegan_encoder.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN15dzgyVg8D"
      },
      "source": [
        "#### `cd` into the repo folder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-L8CaC6O6Bv"
      },
      "outputs": [],
      "source": [
        "cd /content/pytorch_stylegan_encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fpbmESpMRCw"
      },
      "source": [
        "### Get InterFace GAN\n",
        "It should be located inside the newly created folder `pytorch_stylegan_encoder`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZxJhSpN7qoS"
      },
      "outputs": [],
      "source": [
        "!rm -rf InterFaceGAN  # remove the empty directory created by the previous step\n",
        "!git clone https://github.com/genforce/interfacegan.git # clone the repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IiAf-m9MOum"
      },
      "source": [
        "The new directory is named `interfacegan` but `encode_image.py` (to be used later) from `pytorch_stylegan_encoder` requires it to be called `InterFaceGAN`. Thus, we have to rename it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-387tBf7NCvT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.rename('interfacegan', 'InterFaceGAN') # os.rename(old_name, new_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBRbykqEOKVI"
      },
      "source": [
        "### Get StyleGAN-encoder\n",
        "We need this to align the captured images. We will be using the script `align_images.py` which will preprocess images by extracting and aligning faces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbo3XEu4OJoo"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Puzer/stylegan-encoder.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv2Pw_1iUKMO"
      },
      "source": [
        "### Let's see the files inside the repo we just cloned:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glg1mT2aKizL"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXzIm_TqVk2Q"
      },
      "source": [
        "## Folders for our images\n",
        "We need to create folders for our captured and aligned images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5khz8dizO_JB"
      },
      "outputs": [],
      "source": [
        "mkdir aligned_images raw_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEkL_Zqaufcf"
      },
      "source": [
        "# I. Get Images:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb6eYXL7Pm3S"
      },
      "source": [
        "## Some tips for the images:\n",
        "\n",
        "\n",
        "*   Use HD images (preferably > 1000x1000 pixels)\n",
        "*   Make sure your face is not too small\n",
        "*   Neutral expressions & front facing faces will give better results\n",
        "*   Clear, uniform lighting conditions are also recommened\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWHBm38POhJ5"
      },
      "source": [
        "## Option 1: Upload Images manually (usually gives the best results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0veUQL2_OoBr"
      },
      "source": [
        "\n",
        "\n",
        "*   Click the '>' icon in the panel on the top left\n",
        "*   Go to the 'Files' tab\n",
        "*   Unfold the pytorch_stylegan_encoder folder (left-click)\n",
        "*   Right click the 'pytorch_stylegan_encoder/raw_images' folder and click \"upload\"\n",
        "*   I'd recommend starting with 3 - 6 different images containing faces\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToBlvfPgPedc"
      },
      "source": [
        "## Option 2: Take images using your webcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU_R54n2_qYr"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "import io\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "\n",
        "VIDEO_HTML = \"\"\"\n",
        "<video autoplay\n",
        " width=%d height=%d style='cursor: pointer;'></video>\n",
        "<script>\n",
        "\n",
        "var video = document.querySelector('video')\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({ video: true })\n",
        "  .then(stream=> video.srcObject = stream)\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "  video.onclick = ()=>{\n",
        "    var canvas = document.createElement('canvas')\n",
        "    var [w,h] = [video.offsetWidth, video.offsetHeight]\n",
        "    canvas.width = w\n",
        "    canvas.height = h\n",
        "    canvas.getContext('2d')\n",
        "          .drawImage(video, 0, 0, w, h)\n",
        "    video.srcObject.getVideoTracks()[0].stop()\n",
        "    video.replaceWith(canvas)\n",
        "    resolve(canvas.toDataURL('image/jpeg', %f))\n",
        "  }\n",
        "})\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def take_photo(quality=1.0, size=(800,600)):\n",
        "  display(HTML(VIDEO_HTML % (size[0],size[1],quality)))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  f = io.BytesIO(binary)\n",
        "  img = np.asarray(Image.open(f))\n",
        "\n",
        "  timestampStr = datetime.now().strftime(\"%d-%b-%Y (%H:%M:%S.%f)\")\n",
        "  filename = 'raw_images/photo_%s.jpeg' %timestampStr\n",
        "  Image.fromarray(img).save(filename)\n",
        "  print('Image captured and saved to %s' %filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVBAWK0nMjZd"
      },
      "outputs": [],
      "source": [
        "img = take_photo() # click the image to capture a frame!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c0sB3grQ0b9"
      },
      "source": [
        "## Let's check the contents of our image folder before we start:\n",
        "#### (You can always manually delete images by right clicking on them in the file tab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0hakluVJXvO"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "imgs = sorted(os.listdir('raw_images'))\n",
        "\n",
        "print(\"Found %d images in %s\" %(len(imgs), 'raw_images'))\n",
        "if len(imgs) == 0:\n",
        "  print(\"Upload images to the \\\"raw_images\\\" folder!\")\n",
        "else:\n",
        "  print(imgs)\n",
        "\n",
        "for img_path in imgs:\n",
        "  img = Image.open('raw_images/' + img_path)\n",
        "\n",
        "  w,h = img.size\n",
        "  rescale_ratio = 256 / min(w,h)\n",
        "  img = img.resize((int(rescale_ratio*w),int(rescale_ratio*h)), Image.LANCZOS)\n",
        "  display(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb19Jm4fO6bB"
      },
      "source": [
        "## Make sure we're using the right TensorFlow version (1.15):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MM8TtbwK5w8"
      },
      "source": [
        "# II. Auto-Align faces:\n",
        "### This script wil:\n",
        "\n",
        "\n",
        "1.   Look for faces in the images\n",
        "2.   Crop out the faces from the images\n",
        "3.   Align the faces (center the nose and make the eyes horizontal)\n",
        "4.   Rescale the resulting images and save them in \"aligned_images\" folder\n",
        "\n",
        "`align_images.py` is located in the folder `styleganencoder` so we have to give the relative path (styleganencoder/align_images.py) to the file when running it (or `cd` into that folder).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de-TddqU_dY-"
      },
      "outputs": [],
      "source": [
        "!python stylegan-encoder/align_images.py raw_images/ aligned_images/ --output_size=1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB8kxWpBRSMA"
      },
      "source": [
        "## Let's take a look at our aligned images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_Er59x8Mt54"
      },
      "outputs": [],
      "source": [
        "def display_folder_content(folder, res = 256):\n",
        "  if folder[-1] != '/': folder += '/'\n",
        "  for i, img_path in enumerate(sorted(os.listdir(folder))):\n",
        "    if '.png' in img_path:\n",
        "      display(Image.open(folder+img_path).resize((res,res)), 'img %d: %s' %(i, img_path))\n",
        "      print('\\n')\n",
        "\n",
        "display_folder_content('aligned_images')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaEzcVk7RVjI"
      },
      "source": [
        "# Important, before moving on:\n",
        "### Manually clean the `aligned_images` directory\n",
        "\n",
        "> ### 1. Manually remove all 'bad' images that are not faces / don't look sharp / clear\n",
        "> #####  (Use the image names from the plots above to guide you)\n",
        "> ### 2. Make sure you don't have too many faces in this folder (8 at most preferably)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0mtyFkhHRMU"
      },
      "source": [
        "# Encoding faces into StyleGAN latent space:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKryjSOHSrHh"
      },
      "source": [
        "![title](https://raw.githubusercontent.com/pbaylies/stylegan-encoder/master/mona_example.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmHwHBH8QUc7"
      },
      "source": [
        "## Download a pretrained and fine-tuned the ResNet encoder.\n",
        "This model takes an image as input and estimates the corresponding latent code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM_fG3bgdmgn"
      },
      "outputs": [],
      "source": [
        "!gdown https://github.com/jacobhallberg/pytorch_stylegan_encoder/releases/download/v1.0/trained_models.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_pBjZ4KQRC7"
      },
      "source": [
        "The model is called `image_to_latent.py` and is downloaded together with the StyleGAN model (`stylegan_ffhq.pth`) in the archive `trained_models.zip`.\n",
        "\n",
        "We'll extract the models and move them to folders where `encode_images.py` can find them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVdh8LIdQVSz"
      },
      "outputs": [],
      "source": [
        "!unzip trained_models.zip\n",
        "!mv trained_models/stylegan_ffhq.pth InterFaceGAN/models/pretrain/\n",
        "!mv trained_models/image_to_latent.pt ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3Xjrc2hTmMI"
      },
      "source": [
        "# III. The actual encoding: `encode_images.py`\n",
        "\n",
        "#### Note: This script will also download:\n",
        "\n",
        "*   A pretrained VGG-16 network, trained on ImageNet\n",
        "\n",
        "#### After guessing the initial latent codes using the pretrained ResNet, it will run gradient descent to optimize the latent faces!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1WKZNN3vFVv"
      },
      "outputs": [],
      "source": [
        "print(\"aligned_images contains %d images ready for encoding!\" %len(os.listdir('aligned_images/')))\n",
        "print(\"Recommended batch_size for the encode_images process: %d\" %min(len(os.listdir('aligned_images/')), 8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqiCMtQnZJus"
      },
      "source": [
        "### Depending on the settings, the encoding process might take a few minutes..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gueUvzGMZtWi"
      },
      "outputs": [],
      "source": [
        "!python encode_image.py aligned_images/aligned1.png dlatents.npy --save_optimized_image true --use_latent_finder true --image_to_latent_path ./image_to_latent.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsKCGkhjclX_"
      },
      "source": [
        "## Generate images from the latent encodings\n",
        "\n",
        "Let's load the StyleGAN network into memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORuPELQ3_wSc"
      },
      "outputs": [],
      "source": [
        "from InterFaceGAN.models.stylegan_generator import StyleGANGenerator\n",
        "from models.latent_optimizer import PostSynthesisProcessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8_O4dC3AGgG"
      },
      "outputs": [],
      "source": [
        "synthesizer = StyleGANGenerator(\"stylegan_ffhq\").model.synthesis\n",
        "post_processing = PostSynthesisProcessing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohP-sa6JAIL2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "latent = torch.tensor(np.load('dlatents.npy'))\n",
        "latent = latent.to(device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4hYosKAAWej"
      },
      "outputs": [],
      "source": [
        "# Generate/synthezie from latent encoding\n",
        "pred_image = synthesizer(latent)\n",
        "# post process for better image quality\n",
        "pred_image = post_processing(pred_image).detach().cpu().numpy().astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0K94X8DBMld"
      },
      "outputs": [],
      "source": [
        "# pred_image = pred_image.detach().cpu().numpy().astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G2GlNMNBQS1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "original_image = plt.imread('aligned_images/aligned1.png')\n",
        "\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "ax = fig.add_subplot(2,1,1)\n",
        "ax.imshow(original_image)\n",
        "ax.set_title('original image')\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax = fig.add_subplot(2,1,2)\n",
        "pred_image = pred_image.squeeze()\n",
        "ax.imshow(np.transpose(pred_image, (1,2,0)))\n",
        "ax.set_title('predicted image')\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLaJBYEuf09x"
      },
      "source": [
        "## How can we generate random images?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwwHDl-heh6O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFbpOqW5OelA"
      },
      "source": [
        "# IV. Cherry pick images & dump their latent vectors to disk\n",
        "### Manipulating latent vectors (Notebook II) is tricky and will only work well if the face encoding looks 'good'\n",
        "### Cherry pick a few images where the optimization worked well\n",
        "> (Use the image indices from the plot titles above)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGj-xKoYX1TZ"
      },
      "source": [
        "## Save these latent vectors to disk:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zODfZSdhX9MM"
      },
      "source": [
        "# V. Manipulating the faces\n",
        "### Everything we downloaded / saved to disk is currently on a temporary VM running on Google Colab\n",
        "> We'll want to reuse the latent vectors later, so you should download them manually:\n",
        ">> * Go to the root directory using the Files browser\n",
        ">> * Richt-click & Download the latent representations: \"output_vectors.npy\"\n",
        "## Next, let's continue with notebook II:\n",
        "> ### Simply open the second notebook from the Drive folder and continue the guide-steps\n",
        "> ### (Hint: Notebook II is where all the fun is!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFOnhT99eqxY"
      },
      "source": [
        "![alt text](https://66.media.tumblr.com/tumblr_mc3hg5VpQP1qcy0p7o1_400.gif)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}